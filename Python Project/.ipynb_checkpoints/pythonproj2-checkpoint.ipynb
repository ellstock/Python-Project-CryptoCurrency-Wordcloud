{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCKitpZTpISn"
   },
   "source": [
    "# Making necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3D1My5RHncNS"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "cWbZwPp6VBMi",
    "outputId": "62a2e69a-869b-4cb1-83f3-5860e803e2a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/elliot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/elliot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/elliot/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP library imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpT3m7C2o_Qc"
   },
   "source": [
    "# Applying the transformation we've seen to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "V-Lu49mLnke7",
    "outputId": "d5ced2c8-3d38-4309-d0ce-c9bd43d95931"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-557e01cc3cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Creating a new column swith tokenized reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Displaying part of the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-557e01cc3cf7>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(review)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Deleting words with  only one caracter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     return [\n\u001b[1;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m    104\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \"\"\"\n\u001b[0;32m-> 1269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \"\"\"\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \"\"\"\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \"\"\"\n\u001b[1;32m   1353\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \"\"\"\n\u001b[1;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Loading dataFrame\n",
    "df = pd.read_csv(r'cryptodata_test_preprocessed.csv') # requires that the previous step of pre-processing has worked!\n",
    "\n",
    "# Preparing transformations for preprocessing function\n",
    "caracters_to_remove = list(string.punctuation)\n",
    "transformation_car_dict = {initial:\" \" for initial in caracters_to_remove}\n",
    "\n",
    "with_accent = ['é', 'è', 'ê', 'à', 'ù', 'ç', 'ô', 'î']\n",
    "without_accent = ['e', 'e', 'e', 'a', 'u', 'c', 'o', 'i']\n",
    "transformation_accent_dict = {before:after for before, after in zip(with_accent, without_accent)}\n",
    "\n",
    "stopW = stopwords.words('french')\n",
    "stopW += ['les', 'a', 'tout']\n",
    "\n",
    "\n",
    "# Preprocessing function to apply to the content column\n",
    "def preprocessing(review):\n",
    "  \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(review)\n",
    "    \n",
    "    # Deleting words with  only one caracter\n",
    "    tokens = [token for token in tokens if len(token)>2]\n",
    "    \n",
    "    # stopwords + lowercase\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopW]   \n",
    "    \n",
    "    # Removing accents\n",
    "    tokens = [token.translate(str.maketrans(transformation_accent_dict)) for token in tokens]\n",
    "    \n",
    "    # Deleting specific caracters\n",
    "    tokens = [token.translate(str.maketrans(transformation_car_dict)) for token in tokens]\n",
    "        \n",
    "    return tokens\n",
    "  \n",
    "\n",
    "# Creating a new column swith tokenized reviews\n",
    "df['tokens'] = df['body'].apply(preprocessing)\n",
    "\n",
    "# Displaying part of the result\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-WARsCeo-JL"
   },
   "source": [
    "# Discovering [Stemming](https://en.wikipedia.org/wiki/Stemming) and [Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "\n",
    "\n",
    "If you want to understand how the [Porter Algorithm](https://fr.wikipedia.org/wiki/Racinisation#Algorithme_de_Porter) works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-2VUwd4kKL2"
   },
   "source": [
    "## Visualizing the effects of two different stemmers on basic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EHK8XvxD3v_G",
    "outputId": "83c80b3a-2efa-4860-b12e-4bb9119d8b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMc4hDBnkT8n"
   },
   "source": [
    "## Effects on a total sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zVE0VoS4Pp9"
   },
   "outputs": [],
   "source": [
    "def stemSentence(sentence, stemmer):\n",
    "    \n",
    "    token_words = word_tokenize(sentence)\n",
    "    stem_sentence = []\n",
    "    \n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    \n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FZsE-n21lxwN",
    "outputId": "b98b81ae-f78e-4fcb-f355-5869d5e4c1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ar very intellig and work very python and now they ar python their way to success . \n",
      "python are veri intellig and work veri pythonli and now they are python their way to success . \n"
     ]
    }
   ],
   "source": [
    "# And compare differences\n",
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "print(stemSentence(sentence, lancaster))\n",
    "print(stemSentence(sentence, porter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Qcbb5PYKlzCD",
    "outputId": "ec9de27e-88be-4900-e74d-038a1e6dfd9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce matin je sui allé achet un galet à la boulangery pui je me sui régalé av de venir en cour . \n",
      "Ce matin je sui allé achet une galett à la boulangeri pui je me sui régalé avant de venir en cour . \n"
     ]
    }
   ],
   "source": [
    "# Look at what is happening on a french sentence\n",
    "sentence=\"Ce matin je suis allé acheter une galette à la boulangerie puis je me suis régalé avant de venir en cours.\"\n",
    "\n",
    "print(stemSentence(sentence, lancaster))\n",
    "print(stemSentence(sentence, porter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxILJkRnkeZ2"
   },
   "source": [
    "## A stemmer to use on different languages (for example french..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iMi9p7VH4Rbx",
    "outputId": "7a5d169f-762b-481c-931f-840bcba2895b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cet phras est à la fois amus et surpren '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frenchStemSentence(sentence):\n",
    "    frenchStemmer=SnowballStemmer(\"french\", ignore_stopwords=False)\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(frenchStemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "frenchStemSentence(\"cette phrase est à la fois amusante et surprenante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELSBOPDTorCM"
   },
   "source": [
    "## Having a look at lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y3nNOH9FWpM8",
    "outputId": "d52954c5-07b4-4e07-b46e-857ee852db7e"
   },
   "outputs": [],
   "source": [
    "# Initiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create lematizing function\n",
    "def lemmatize(sentence):\n",
    "    tokens=word_tokenize(sentence)\n",
    "    tokens = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(token,pos='a'),pos='v'),pos='n') for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# And display results\n",
    "lemmer = lemmatize(\"Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Such an analysis can reveal feature that be not easily visible from the variation in the individual gene and can lead to a picture of expression that be more biologically transparent'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUN-NholwS3a"
   },
   "source": [
    "# Applying one of those modification to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VP7Tn1Uy4tj"
   },
   "source": [
    " **Preparing both functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14FnlWXwoum9"
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    tokens = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(token,pos='a'),pos='v'),pos='n') for token in tokens]\n",
    "    return tokens  \n",
    "\n",
    "# Stemming\n",
    "frenchStemmer=SnowballStemmer(\"french\")\n",
    "def stem(tokens):\n",
    "    tokens = [frenchStemmer.stem(token) for token in tokens]\n",
    "    return tokens  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZHHZdbjzBlI"
   },
   "source": [
    "**Selecting which one to apply, given the language used in your reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZIv9DttzLml"
   },
   "outputs": [],
   "source": [
    "# Are your reviews in English ? (here it is unfortunately not the case)\n",
    "english = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9VkIu1_zO2Z"
   },
   "source": [
    "**And finally applying it to our dataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "3QRw4_nLySif",
    "outputId": "c2bd7108-eb38-4b0a-b388-92eeef98393e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel_name</th>\n",
       "      <th>published_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>trip_date</th>\n",
       "      <th>tokens</th>\n",
       "      <th>review_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>27 septembre 2018</td>\n",
       "      <td>4</td>\n",
       "      <td>Après une désastreuse aventure au Bois franc, ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>très bon week end</td>\n",
       "      <td>août 2018</td>\n",
       "      <td>[apres, desastreuse, aventure, bois, franc, qu...</td>\n",
       "      <td>[apre, desastr, aventur, bois, franc, quel, pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>18 janvier 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>Ambiance détendue , une vraie déconnexion.  Le...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Séjour agréable comme toujours ,une vraie déco...</td>\n",
       "      <td>mars 2018</td>\n",
       "      <td>[ambiance, detendue, vraie, deconnexion, logem...</td>\n",
       "      <td>[ambianc, detendu, vrai, deconnexion, log, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>11 novembre 2018</td>\n",
       "      <td>3</td>\n",
       "      <td>Première fois que nous allions à center Parcs ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>3,5 serait plus juste</td>\n",
       "      <td>juillet 2018</td>\n",
       "      <td>[premiere, fois, allions, center, parcs, ailet...</td>\n",
       "      <td>[premier, fois, allion, cent, parc, ailet, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>3 octobre 2018</td>\n",
       "      <td>4</td>\n",
       "      <td>Génial pour les enfants petits et grands ! Ple...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Endroit sympathique</td>\n",
       "      <td>avril 2018</td>\n",
       "      <td>[genial, enfants, petits, grands, pleins, d ac...</td>\n",
       "      <td>[genial, enfant, petit, grand, plein, d activi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>17 janvier 2019</td>\n",
       "      <td>2</td>\n",
       "      <td>Nous avons fait une réservation avec notre CE ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Réservation février 2019</td>\n",
       "      <td>janvier 2019</td>\n",
       "      <td>[fait, reservation, novembre, 2018, cottage, p...</td>\n",
       "      <td>[fait, reserv, novembr, 2018, cottag, person, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      hotel_name     published_date  rating  \\\n",
       "0  Center Parcs Le Lac d'Ailette  27 septembre 2018       4   \n",
       "1  Center Parcs Le Lac d'Ailette    18 janvier 2019       5   \n",
       "2  Center Parcs Le Lac d'Ailette   11 novembre 2018       3   \n",
       "3  Center Parcs Le Lac d'Ailette     3 octobre 2018       4   \n",
       "4  Center Parcs Le Lac d'Ailette    17 janvier 2019       2   \n",
       "\n",
       "                                              review language  \\\n",
       "0  Après une désastreuse aventure au Bois franc, ...       fr   \n",
       "1  Ambiance détendue , une vraie déconnexion.  Le...       fr   \n",
       "2  Première fois que nous allions à center Parcs ...       fr   \n",
       "3  Génial pour les enfants petits et grands ! Ple...       fr   \n",
       "4  Nous avons fait une réservation avec notre CE ...       fr   \n",
       "\n",
       "                                               title      trip_date  \\\n",
       "0                                  très bon week end      août 2018   \n",
       "1  Séjour agréable comme toujours ,une vraie déco...      mars 2018   \n",
       "2                             3,5 serait plus juste    juillet 2018   \n",
       "3                                Endroit sympathique     avril 2018   \n",
       "4                           Réservation février 2019   janvier 2019   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [apres, desastreuse, aventure, bois, franc, qu...   \n",
       "1  [ambiance, detendue, vraie, deconnexion, logem...   \n",
       "2  [premiere, fois, allions, center, parcs, ailet...   \n",
       "3  [genial, enfants, petits, grands, pleins, d ac...   \n",
       "4  [fait, reservation, novembre, 2018, cottage, p...   \n",
       "\n",
       "                                   review_lemmatized  \n",
       "0  [apre, desastr, aventur, bois, franc, quel, pl...  \n",
       "1  [ambianc, detendu, vrai, deconnexion, log, pro...  \n",
       "2  [premier, fois, allion, cent, parc, ailet, pre...  \n",
       "3  [genial, enfant, petit, grand, plein, d activi...  \n",
       "4  [fait, reserv, novembr, 2018, cottag, person, ...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making appropriate modification\n",
    "if english:\n",
    "    df['review_lemmatized'] = df['tokens'].apply(lemmatize)\n",
    "\n",
    "else:\n",
    "    df['review_lemmatized'] = df['tokens'].apply(stem)\n",
    "\n",
    "# And displaying results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6faNnlpH3L8-"
   },
   "source": [
    "# Final modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "wsQgbDCUpSGG",
    "outputId": "7b238486-e3c6-468c-98d5-d2c693e723b5"
   },
   "outputs": [],
   "source": [
    "# Why not doing the same on title \n",
    "df['title_lemmatized'] = df['title'].apply(preprocessing).apply(stem)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally keeping only necessary columns\n",
    "del(df['content'])\n",
    "del(df['tokens'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel_name</th>\n",
       "      <th>published_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>trip_date</th>\n",
       "      <th>tokens</th>\n",
       "      <th>review_lemmatized</th>\n",
       "      <th>title_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>27 septembre 2018</td>\n",
       "      <td>4</td>\n",
       "      <td>Après une désastreuse aventure au Bois franc, ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>très bon week end</td>\n",
       "      <td>août 2018</td>\n",
       "      <td>[apres, desastreuse, aventure, bois, franc, qu...</td>\n",
       "      <td>[apre, desastr, aventur, bois, franc, quel, pl...</td>\n",
       "      <td>[tre, bon, week, end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>18 janvier 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>Ambiance détendue , une vraie déconnexion.  Le...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Séjour agréable comme toujours ,une vraie déco...</td>\n",
       "      <td>mars 2018</td>\n",
       "      <td>[ambiance, detendue, vraie, deconnexion, logem...</td>\n",
       "      <td>[ambianc, detendu, vrai, deconnexion, log, pro...</td>\n",
       "      <td>[sejour, agreabl, comm, toujour, vrai, deconne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>11 novembre 2018</td>\n",
       "      <td>3</td>\n",
       "      <td>Première fois que nous allions à center Parcs ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>3,5 serait plus juste</td>\n",
       "      <td>juillet 2018</td>\n",
       "      <td>[premiere, fois, allions, center, parcs, ailet...</td>\n",
       "      <td>[premier, fois, allion, cent, parc, ailet, pre...</td>\n",
       "      <td>[3 5, plus, just]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>3 octobre 2018</td>\n",
       "      <td>4</td>\n",
       "      <td>Génial pour les enfants petits et grands ! Ple...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Endroit sympathique</td>\n",
       "      <td>avril 2018</td>\n",
       "      <td>[genial, enfants, petits, grands, pleins, d ac...</td>\n",
       "      <td>[genial, enfant, petit, grand, plein, d activi...</td>\n",
       "      <td>[endroit, sympath]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>17 janvier 2019</td>\n",
       "      <td>2</td>\n",
       "      <td>Nous avons fait une réservation avec notre CE ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Réservation février 2019</td>\n",
       "      <td>janvier 2019</td>\n",
       "      <td>[fait, reservation, novembre, 2018, cottage, p...</td>\n",
       "      <td>[fait, reserv, novembr, 2018, cottag, person, ...</td>\n",
       "      <td>[reserv, fevri, 2019]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>August 31, 2018</td>\n",
       "      <td>4</td>\n",
       "      <td>This was the third European CP we've visited, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Great holiday, but too many cars on site</td>\n",
       "      <td>August 2018</td>\n",
       "      <td>[this, was, the, third, european,  ve, visited...</td>\n",
       "      <td>[this, was, the, third, european,  ve, visited...</td>\n",
       "      <td>[great, holiday, but, too, many, car, sit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>September 1, 2018</td>\n",
       "      <td>5</td>\n",
       "      <td>I have to admit after reading so many negative...</td>\n",
       "      <td>en</td>\n",
       "      <td>Supassed expectations</td>\n",
       "      <td>August 2018</td>\n",
       "      <td>[have, admit, after, reading, many, negative, ...</td>\n",
       "      <td>[hav, admit, after, reading, many, negat, revi...</td>\n",
       "      <td>[supassed, expect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>August 31, 2018</td>\n",
       "      <td>3</td>\n",
       "      <td>they have good amenities but the cottages are ...</td>\n",
       "      <td>en</td>\n",
       "      <td>good amenities but the cottages are not clean</td>\n",
       "      <td>August 2018</td>\n",
       "      <td>[they, have, good, amenities, but, the, cottag...</td>\n",
       "      <td>[they, hav, good, amenit, but, the, cottag, ar...</td>\n",
       "      <td>[good, amenit, but, the, cottag, are, not, clean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>September 1, 2018</td>\n",
       "      <td>3</td>\n",
       "      <td>Pool was fun but there was overcrowding at tim...</td>\n",
       "      <td>en</td>\n",
       "      <td>Not enamoured with Le Lac D'Ailette</td>\n",
       "      <td>August 2018</td>\n",
       "      <td>[pool, was, fun, but, there, was, overcrowding...</td>\n",
       "      <td>[pool, was, fun, but, ther, was, overcrowding,...</td>\n",
       "      <td>[not, enamoured, with, lac, d ailet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Center Parcs Le Lac d'Ailette</td>\n",
       "      <td>September 1, 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>Our third European Centre Parcs.  The location...</td>\n",
       "      <td>en</td>\n",
       "      <td>Family holiday - children 10 and 7</td>\n",
       "      <td>August 2018</td>\n",
       "      <td>[our, third, european, centre, parcs, the, loc...</td>\n",
       "      <td>[our, third, european, centr, parc, the, locat...</td>\n",
       "      <td>[family, holiday, children, and]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      hotel_name     published_date  rating  \\\n",
       "0  Center Parcs Le Lac d'Ailette  27 septembre 2018       4   \n",
       "1  Center Parcs Le Lac d'Ailette    18 janvier 2019       5   \n",
       "2  Center Parcs Le Lac d'Ailette   11 novembre 2018       3   \n",
       "3  Center Parcs Le Lac d'Ailette     3 octobre 2018       4   \n",
       "4  Center Parcs Le Lac d'Ailette    17 janvier 2019       2   \n",
       "5  Center Parcs Le Lac d'Ailette    August 31, 2018       4   \n",
       "6  Center Parcs Le Lac d'Ailette  September 1, 2018       5   \n",
       "7  Center Parcs Le Lac d'Ailette    August 31, 2018       3   \n",
       "8  Center Parcs Le Lac d'Ailette  September 1, 2018       3   \n",
       "9  Center Parcs Le Lac d'Ailette  September 1, 2018       2   \n",
       "\n",
       "                                              review language  \\\n",
       "0  Après une désastreuse aventure au Bois franc, ...       fr   \n",
       "1  Ambiance détendue , une vraie déconnexion.  Le...       fr   \n",
       "2  Première fois que nous allions à center Parcs ...       fr   \n",
       "3  Génial pour les enfants petits et grands ! Ple...       fr   \n",
       "4  Nous avons fait une réservation avec notre CE ...       fr   \n",
       "5  This was the third European CP we've visited, ...       en   \n",
       "6  I have to admit after reading so many negative...       en   \n",
       "7  they have good amenities but the cottages are ...       en   \n",
       "8  Pool was fun but there was overcrowding at tim...       en   \n",
       "9  Our third European Centre Parcs.  The location...       en   \n",
       "\n",
       "                                               title      trip_date  \\\n",
       "0                                  très bon week end      août 2018   \n",
       "1  Séjour agréable comme toujours ,une vraie déco...      mars 2018   \n",
       "2                             3,5 serait plus juste    juillet 2018   \n",
       "3                                Endroit sympathique     avril 2018   \n",
       "4                           Réservation février 2019   janvier 2019   \n",
       "5           Great holiday, but too many cars on site    August 2018   \n",
       "6                              Supassed expectations    August 2018   \n",
       "7      good amenities but the cottages are not clean    August 2018   \n",
       "8                Not enamoured with Le Lac D'Ailette    August 2018   \n",
       "9                 Family holiday - children 10 and 7    August 2018   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [apres, desastreuse, aventure, bois, franc, qu...   \n",
       "1  [ambiance, detendue, vraie, deconnexion, logem...   \n",
       "2  [premiere, fois, allions, center, parcs, ailet...   \n",
       "3  [genial, enfants, petits, grands, pleins, d ac...   \n",
       "4  [fait, reservation, novembre, 2018, cottage, p...   \n",
       "5  [this, was, the, third, european,  ve, visited...   \n",
       "6  [have, admit, after, reading, many, negative, ...   \n",
       "7  [they, have, good, amenities, but, the, cottag...   \n",
       "8  [pool, was, fun, but, there, was, overcrowding...   \n",
       "9  [our, third, european, centre, parcs, the, loc...   \n",
       "\n",
       "                                   review_lemmatized  \\\n",
       "0  [apre, desastr, aventur, bois, franc, quel, pl...   \n",
       "1  [ambianc, detendu, vrai, deconnexion, log, pro...   \n",
       "2  [premier, fois, allion, cent, parc, ailet, pre...   \n",
       "3  [genial, enfant, petit, grand, plein, d activi...   \n",
       "4  [fait, reserv, novembr, 2018, cottag, person, ...   \n",
       "5  [this, was, the, third, european,  ve, visited...   \n",
       "6  [hav, admit, after, reading, many, negat, revi...   \n",
       "7  [they, hav, good, amenit, but, the, cottag, ar...   \n",
       "8  [pool, was, fun, but, ther, was, overcrowding,...   \n",
       "9  [our, third, european, centr, parc, the, locat...   \n",
       "\n",
       "                                    title_lemmatized  \n",
       "0                              [tre, bon, week, end]  \n",
       "1  [sejour, agreabl, comm, toujour, vrai, deconne...  \n",
       "2                                  [3 5, plus, just]  \n",
       "3                                 [endroit, sympath]  \n",
       "4                              [reserv, fevri, 2019]  \n",
       "5         [great, holiday, but, too, many, car, sit]  \n",
       "6                                 [supassed, expect]  \n",
       "7  [good, amenit, but, the, cottag, are, not, clean]  \n",
       "8               [not, enamoured, with, lac, d ailet]  \n",
       "9                   [family, holiday, children, and]  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv (r'C:\\Users\\Ellio\\Desktop\\clean_data_group_5.csv', index=False, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Après une désastreuse aventure au Bois franc, quel plaisir de retourner au lac d'Ailette..en plus les cottages ont été rénovés avec goût, Partis début août sous la canicule, nous avons apprécié les plaisirs de l'aquamondo malgré l'affluence,(peut être faudrait-il que les personnes ne séjournant pas dans le parc n'aient plus accès aux installations quand celui-ci est déjà complet - 1700 personnes dans la piscine c'est vraiment beaucoup trop)Nous avons loué un cottage premium bord de lac . Nous avons aimés nos soirées tranquilles sur la terrasse; beaucoup moins la chaleur dans le cottage (ventilation à prévoir)  celui-ci était très propre à notre arrivée et les quelques soucis rencontrés (pas de pile dans la télécommande, sèche-linge cassé) ont été très vite réglés par un personnel compétant  De quoi bien commencer notre séjour. Un bel endroit où nous reviendrons certainement l'été prochain\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "X_HEC_Session_3_Notebook_2_stemming.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
