{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCKitpZTpISn"
   },
   "source": [
    "# Making necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3D1My5RHncNS"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "cWbZwPp6VBMi",
    "outputId": "62a2e69a-869b-4cb1-83f3-5860e803e2a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/elliot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/elliot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/elliot/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP library imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpT3m7C2o_Qc"
   },
   "source": [
    "# Applying the transformation we've seen to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "V-Lu49mLnke7",
    "outputId": "d5ced2c8-3d38-4309-d0ce-c9bd43d95931"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>body</th>\n",
       "      <th>stopword_reviews</th>\n",
       "      <th>body_as_str</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1543420943</td>\n",
       "      <td>The new broker-dealer and end-user oriented pr...</td>\n",
       "      <td>['new', 'broker', 'dealer', 'end', 'user', 'or...</td>\n",
       "      <td>The new broker-dealer and end-user oriented pr...</td>\n",
       "      <td>[the, new, broker dealer, and, end user, orien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1543420851</td>\n",
       "      <td>Tapatalk, the mobile forum application with an...</td>\n",
       "      <td>['tapatalk', 'mobile', 'forum', 'application',...</td>\n",
       "      <td>Tapatalk, the mobile forum application with an...</td>\n",
       "      <td>[tapatalk, the, mobile, forum, application, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1543420803</td>\n",
       "      <td>SEC indecision has led companies including sev...</td>\n",
       "      <td>['sec', 'indecision', 'led', 'companies', 'inc...</td>\n",
       "      <td>SEC indecision has led companies including sev...</td>\n",
       "      <td>[sec, indecision, has, led, companies, includi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1543420803</td>\n",
       "      <td>The chairman of the U.S. Securities and Exchan...</td>\n",
       "      <td>['chairman', 'u', 'securities', 'exchange', 'c...</td>\n",
       "      <td>The chairman of the U.S. Securities and Exchan...</td>\n",
       "      <td>[the, chairman, the, u s , securities, and, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1543420552</td>\n",
       "      <td>KNOMAD, the Global Knowledge Partnership onRea...</td>\n",
       "      <td>['knomad', 'global', 'knowledge', 'partnership...</td>\n",
       "      <td>KNOMAD, the Global Knowledge Partnership onRea...</td>\n",
       "      <td>[knomad, the, global, knowledge, partnership, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           time                                               body  \\\n",
       "495  1543420943  The new broker-dealer and end-user oriented pr...   \n",
       "496  1543420851  Tapatalk, the mobile forum application with an...   \n",
       "497  1543420803  SEC indecision has led companies including sev...   \n",
       "498  1543420803  The chairman of the U.S. Securities and Exchan...   \n",
       "499  1543420552  KNOMAD, the Global Knowledge Partnership onRea...   \n",
       "\n",
       "                                      stopword_reviews  \\\n",
       "495  ['new', 'broker', 'dealer', 'end', 'user', 'or...   \n",
       "496  ['tapatalk', 'mobile', 'forum', 'application',...   \n",
       "497  ['sec', 'indecision', 'led', 'companies', 'inc...   \n",
       "498  ['chairman', 'u', 'securities', 'exchange', 'c...   \n",
       "499  ['knomad', 'global', 'knowledge', 'partnership...   \n",
       "\n",
       "                                           body_as_str  \\\n",
       "495  The new broker-dealer and end-user oriented pr...   \n",
       "496  Tapatalk, the mobile forum application with an...   \n",
       "497  SEC indecision has led companies including sev...   \n",
       "498  The chairman of the U.S. Securities and Exchan...   \n",
       "499  KNOMAD, the Global Knowledge Partnership onRea...   \n",
       "\n",
       "                                                tokens  \n",
       "495  [the, new, broker dealer, and, end user, orien...  \n",
       "496  [tapatalk, the, mobile, forum, application, wi...  \n",
       "497  [sec, indecision, has, led, companies, includi...  \n",
       "498  [the, chairman, the, u s , securities, and, ex...  \n",
       "499  [knomad, the, global, knowledge, partnership, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataFrame\n",
    "df = pd.read_csv(r'cryptodata_test_preprocessed.csv') # requires that the previous step of pre-processing has worked!\n",
    "\n",
    "# Preparing transformations for preprocessing function\n",
    "caracters_to_remove = list(string.punctuation)\n",
    "transformation_car_dict = {initial:\" \" for initial in caracters_to_remove}\n",
    "\n",
    "with_accent = ['é', 'è', 'ê', 'à', 'ù', 'ç', 'ô', 'î']\n",
    "without_accent = ['e', 'e', 'e', 'a', 'u', 'c', 'o', 'i']\n",
    "transformation_accent_dict = {before:after for before, after in zip(with_accent, without_accent)}\n",
    "\n",
    "stopW = stopwords.words('french')\n",
    "stopW += ['les', 'a', 'tout']\n",
    "\n",
    "\n",
    "# Preprocessing function to apply to the content column\n",
    "def preprocessing(review):\n",
    "  \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(review)\n",
    "    \n",
    "    # Deleting words with  only one caracter\n",
    "    tokens = [token for token in tokens if len(token)>2]\n",
    "    \n",
    "    # stopwords + lowercase\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopW]   \n",
    "    \n",
    "    # Removing accents\n",
    "    tokens = [token.translate(str.maketrans(transformation_accent_dict)) for token in tokens]\n",
    "    \n",
    "    # Deleting specific caracters\n",
    "    tokens = [token.translate(str.maketrans(transformation_car_dict)) for token in tokens]\n",
    "        \n",
    "    return tokens\n",
    "  \n",
    "\n",
    "# Creating a new column swith tokenized reviews\n",
    "df['tokens'] = df['body'].apply(str).apply(preprocessing)\n",
    "\n",
    "\n",
    "# Displaying part of the result\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-WARsCeo-JL"
   },
   "source": [
    "# Discovering [Stemming](https://en.wikipedia.org/wiki/Stemming) and [Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "\n",
    "\n",
    "If you want to understand how the [Porter Algorithm](https://fr.wikipedia.org/wiki/Racinisation#Algorithme_de_Porter) works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-2VUwd4kKL2"
   },
   "source": [
    "## Visualizing the effects of two different stemmers on basic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EHK8XvxD3v_G",
    "outputId": "83c80b3a-2efa-4860-b12e-4bb9119d8b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMc4hDBnkT8n"
   },
   "source": [
    "## Effects on a total sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zVE0VoS4Pp9"
   },
   "outputs": [],
   "source": [
    "def stemSentence(sentence, stemmer):\n",
    "    \n",
    "    token_words = word_tokenize(sentence)\n",
    "    stem_sentence = []\n",
    "    \n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    \n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FZsE-n21lxwN",
    "outputId": "b98b81ae-f78e-4fcb-f355-5869d5e4c1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ar very intellig and work very python and now they ar python their way to success . \n",
      "python are veri intellig and work veri pythonli and now they are python their way to success . \n"
     ]
    }
   ],
   "source": [
    "# And compare differences\n",
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "print(stemSentence(sentence, lancaster))\n",
    "print(stemSentence(sentence, porter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Qcbb5PYKlzCD",
    "outputId": "ec9de27e-88be-4900-e74d-038a1e6dfd9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce matin je sui allé achet un galet à la boulangery pui je me sui régalé av de venir en cour . \n",
      "Ce matin je sui allé achet une galett à la boulangeri pui je me sui régalé avant de venir en cour . \n"
     ]
    }
   ],
   "source": [
    "# Look at what is happening on a french sentence\n",
    "sentence=\"Ce matin je suis allé acheter une galette à la boulangerie puis je me suis régalé avant de venir en cours.\"\n",
    "\n",
    "print(stemSentence(sentence, lancaster))\n",
    "print(stemSentence(sentence, porter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxILJkRnkeZ2"
   },
   "source": [
    "## A stemmer to use on different languages (for example french..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iMi9p7VH4Rbx",
    "outputId": "7a5d169f-762b-481c-931f-840bcba2895b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cet phras est à la fois amus et surpren '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frenchStemSentence(sentence):\n",
    "    frenchStemmer=SnowballStemmer(\"french\", ignore_stopwords=False)\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(frenchStemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "frenchStemSentence(\"cette phrase est à la fois amusante et surprenante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELSBOPDTorCM"
   },
   "source": [
    "## Having a look at lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y3nNOH9FWpM8",
    "outputId": "d52954c5-07b4-4e07-b46e-857ee852db7e"
   },
   "outputs": [],
   "source": [
    "# Initiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create lematizing function\n",
    "def lemmatize(sentence):\n",
    "    tokens=word_tokenize(sentence)\n",
    "    tokens = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(token,pos='a'),pos='v'),pos='n') for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# And display results\n",
    "lemmer = lemmatize(\"Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Such an analysis can reveal feature that be not easily visible from the variation in the individual gene and can lead to a picture of expression that be more biologically transparent'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUN-NholwS3a"
   },
   "source": [
    "# Applying one of those modification to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VP7Tn1Uy4tj"
   },
   "source": [
    " **Preparing both functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14FnlWXwoum9"
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    tokens = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(token,pos='a'),pos='v'),pos='n') for token in tokens]\n",
    "    return tokens  \n",
    "\n",
    "# Stemming\n",
    "frenchStemmer=SnowballStemmer(\"french\")\n",
    "def stem(tokens):\n",
    "    tokens = [frenchStemmer.stem(token) for token in tokens]\n",
    "    return tokens  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZHHZdbjzBlI"
   },
   "source": [
    "**Selecting which one to apply, given the language used in your reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZIv9DttzLml"
   },
   "outputs": [],
   "source": [
    "# Are your reviews in English ? (here it is unfortunately not the case)\n",
    "english = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9VkIu1_zO2Z"
   },
   "source": [
    "**And finally applying it to our dataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "3QRw4_nLySif",
    "outputId": "c2bd7108-eb38-4b0a-b388-92eeef98393e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>body</th>\n",
       "      <th>stopword_reviews</th>\n",
       "      <th>body_as_str</th>\n",
       "      <th>tokens</th>\n",
       "      <th>review_lemmatized</th>\n",
       "      <th>body_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1543622368</td>\n",
       "      <td>CoinSpeakerProBit: Professional Digital Curren...</td>\n",
       "      <td>['coinspeakerprobit', 'professional', 'digital...</td>\n",
       "      <td>CoinSpeakerProBit: Professional Digital Curren...</td>\n",
       "      <td>[coinspeakerprobit, professional, digital, cur...</td>\n",
       "      <td>[coinspeakerprob, professional, digital, curre...</td>\n",
       "      <td>[coinspeakerprob, professional, digital, curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1543620634</td>\n",
       "      <td>Today in crypto heard rumors of Satoshi's retu...</td>\n",
       "      <td>['today', 'crypto', 'heard', 'rumors', 'satosh...</td>\n",
       "      <td>Today in crypto heard rumors of Satoshi's retu...</td>\n",
       "      <td>[today, crypto, heard, rumors, satoshi, return...</td>\n",
       "      <td>[today, crypto, heard, rumor, satosh, return, ...</td>\n",
       "      <td>[today, crypto, heard, rumor, satosh, return, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1543620300</td>\n",
       "      <td>Crashes in the crypto market resulted in spike...</td>\n",
       "      <td>['crashes', 'crypto', 'market', 'resulted', 's...</td>\n",
       "      <td>Crashes in the crypto market resulted in spike...</td>\n",
       "      <td>[crashes, the, crypto, market, resulted, spike...</td>\n",
       "      <td>[crash, the, crypto, market, resulted, spik, m...</td>\n",
       "      <td>[crash, the, crypto, market, resulted, spik, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1543620202</td>\n",
       "      <td>SEC Chairman Jay Clayton has claimed that bitc...</td>\n",
       "      <td>['sec', 'chairman', 'jay', 'clayton', 'claimed...</td>\n",
       "      <td>SEC Chairman Jay Clayton has claimed that bitc...</td>\n",
       "      <td>[sec, chairman, jay, clayton, has, claimed, th...</td>\n",
       "      <td>[sec, chairman, jay, clayton, has, claimed, th...</td>\n",
       "      <td>[sec, chairman, jay, clayton, has, claimed, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1543618813</td>\n",
       "      <td>There are four different Ethereum working grou...</td>\n",
       "      <td>['four', 'different', 'ethereum', 'working', '...</td>\n",
       "      <td>There are four different Ethereum working grou...</td>\n",
       "      <td>[there, are, four, different, ethereum, workin...</td>\n",
       "      <td>[ther, are, four, different, ethereum, working...</td>\n",
       "      <td>[ther, are, four, different, ethereum, working...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time                                               body  \\\n",
       "0  1543622368  CoinSpeakerProBit: Professional Digital Curren...   \n",
       "1  1543620634  Today in crypto heard rumors of Satoshi's retu...   \n",
       "2  1543620300  Crashes in the crypto market resulted in spike...   \n",
       "3  1543620202  SEC Chairman Jay Clayton has claimed that bitc...   \n",
       "4  1543618813  There are four different Ethereum working grou...   \n",
       "\n",
       "                                    stopword_reviews  \\\n",
       "0  ['coinspeakerprobit', 'professional', 'digital...   \n",
       "1  ['today', 'crypto', 'heard', 'rumors', 'satosh...   \n",
       "2  ['crashes', 'crypto', 'market', 'resulted', 's...   \n",
       "3  ['sec', 'chairman', 'jay', 'clayton', 'claimed...   \n",
       "4  ['four', 'different', 'ethereum', 'working', '...   \n",
       "\n",
       "                                         body_as_str  \\\n",
       "0  CoinSpeakerProBit: Professional Digital Curren...   \n",
       "1  Today in crypto heard rumors of Satoshi's retu...   \n",
       "2  Crashes in the crypto market resulted in spike...   \n",
       "3  SEC Chairman Jay Clayton has claimed that bitc...   \n",
       "4  There are four different Ethereum working grou...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [coinspeakerprobit, professional, digital, cur...   \n",
       "1  [today, crypto, heard, rumors, satoshi, return...   \n",
       "2  [crashes, the, crypto, market, resulted, spike...   \n",
       "3  [sec, chairman, jay, clayton, has, claimed, th...   \n",
       "4  [there, are, four, different, ethereum, workin...   \n",
       "\n",
       "                                   review_lemmatized  \\\n",
       "0  [coinspeakerprob, professional, digital, curre...   \n",
       "1  [today, crypto, heard, rumor, satosh, return, ...   \n",
       "2  [crash, the, crypto, market, resulted, spik, m...   \n",
       "3  [sec, chairman, jay, clayton, has, claimed, th...   \n",
       "4  [ther, are, four, different, ethereum, working...   \n",
       "\n",
       "                                     body_lemmatized  \n",
       "0  [coinspeakerprob, professional, digital, curre...  \n",
       "1  [today, crypto, heard, rumor, satosh, return, ...  \n",
       "2  [crash, the, crypto, market, resulted, spik, m...  \n",
       "3  [sec, chairman, jay, clayton, has, claimed, th...  \n",
       "4  [ther, are, four, different, ethereum, working...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making appropriate modification\n",
    "if english:\n",
    "    df['body_lemmatized'] = df['tokens'].apply(lemmatize)\n",
    "\n",
    "else:\n",
    "    df['body_lemmatized'] = df['tokens'].apply(stem)\n",
    "\n",
    "# And displaying results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6faNnlpH3L8-"
   },
   "source": [
    "# Final modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "wsQgbDCUpSGG",
    "outputId": "7b238486-e3c6-468c-98d5-d2c693e723b5"
   },
   "outputs": [],
   "source": [
    "# Why not doing the same on title \n",
    "df['title_lemmatized'] = df['title'].apply(preprocessing).apply(stem)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally keeping only necessary columns\n",
    "del(df['content'])\n",
    "del(df['tokens'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>body</th>\n",
       "      <th>stopword_reviews</th>\n",
       "      <th>body_as_str</th>\n",
       "      <th>tokens</th>\n",
       "      <th>review_lemmatized</th>\n",
       "      <th>body_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1543622368</td>\n",
       "      <td>CoinSpeakerProBit: Professional Digital Curren...</td>\n",
       "      <td>['coinspeakerprobit', 'professional', 'digital...</td>\n",
       "      <td>CoinSpeakerProBit: Professional Digital Curren...</td>\n",
       "      <td>[coinspeakerprobit, professional, digital, cur...</td>\n",
       "      <td>[coinspeakerprob, professional, digital, curre...</td>\n",
       "      <td>[coinspeakerprob, professional, digital, curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1543620634</td>\n",
       "      <td>Today in crypto heard rumors of Satoshi's retu...</td>\n",
       "      <td>['today', 'crypto', 'heard', 'rumors', 'satosh...</td>\n",
       "      <td>Today in crypto heard rumors of Satoshi's retu...</td>\n",
       "      <td>[today, crypto, heard, rumors, satoshi, return...</td>\n",
       "      <td>[today, crypto, heard, rumor, satosh, return, ...</td>\n",
       "      <td>[today, crypto, heard, rumor, satosh, return, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1543620300</td>\n",
       "      <td>Crashes in the crypto market resulted in spike...</td>\n",
       "      <td>['crashes', 'crypto', 'market', 'resulted', 's...</td>\n",
       "      <td>Crashes in the crypto market resulted in spike...</td>\n",
       "      <td>[crashes, the, crypto, market, resulted, spike...</td>\n",
       "      <td>[crash, the, crypto, market, resulted, spik, m...</td>\n",
       "      <td>[crash, the, crypto, market, resulted, spik, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1543620202</td>\n",
       "      <td>SEC Chairman Jay Clayton has claimed that bitc...</td>\n",
       "      <td>['sec', 'chairman', 'jay', 'clayton', 'claimed...</td>\n",
       "      <td>SEC Chairman Jay Clayton has claimed that bitc...</td>\n",
       "      <td>[sec, chairman, jay, clayton, has, claimed, th...</td>\n",
       "      <td>[sec, chairman, jay, clayton, has, claimed, th...</td>\n",
       "      <td>[sec, chairman, jay, clayton, has, claimed, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1543618813</td>\n",
       "      <td>There are four different Ethereum working grou...</td>\n",
       "      <td>['four', 'different', 'ethereum', 'working', '...</td>\n",
       "      <td>There are four different Ethereum working grou...</td>\n",
       "      <td>[there, are, four, different, ethereum, workin...</td>\n",
       "      <td>[ther, are, four, different, ethereum, working...</td>\n",
       "      <td>[ther, are, four, different, ethereum, working...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1543617903</td>\n",
       "      <td>On Nov. 29, Sirin Labs announced the commercia...</td>\n",
       "      <td>['nov', '29', 'sirin', 'labs', 'announced', 'c...</td>\n",
       "      <td>On Nov. 29, Sirin Labs announced the commercia...</td>\n",
       "      <td>[nov , sirin, labs, announced, the, commercial...</td>\n",
       "      <td>[nov , sirin, lab, announced, the, commercial,...</td>\n",
       "      <td>[nov , sirin, lab, announced, the, commercial,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1543616700</td>\n",
       "      <td>Switzerlands oldest university, the University...</td>\n",
       "      <td>['switzerlands', 'oldest', 'university', 'univ...</td>\n",
       "      <td>Switzerlands oldest university, the University...</td>\n",
       "      <td>[switzerlands, oldest, university, the, univer...</td>\n",
       "      <td>[switzerland, oldest, university, the, univers...</td>\n",
       "      <td>[switzerland, oldest, university, the, univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1543616305</td>\n",
       "      <td>The newer HBUS cryptocurrency exchange has tak...</td>\n",
       "      <td>['newer', 'hbus', 'cryptocurrency', 'exchange'...</td>\n",
       "      <td>The newer HBUS cryptocurrency exchange has tak...</td>\n",
       "      <td>[the, newer, hbus, cryptocurrency, exchange, h...</td>\n",
       "      <td>[the, new, hbus, cryptocurrency, exchang, has,...</td>\n",
       "      <td>[the, new, hbus, cryptocurrency, exchang, has,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1543616240</td>\n",
       "      <td>Having met the U.S. Se...</td>\n",
       "      <td>['met', 'u', 'securities', 'exchange', 'commis...</td>\n",
       "      <td>Having met the U.S. Se...</td>\n",
       "      <td>[having, met, the, u s , securities, and, exch...</td>\n",
       "      <td>[having, met, the, u s , securit, and, exchang...</td>\n",
       "      <td>[having, met, the, u s , securit, and, exchang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1543615852</td>\n",
       "      <td>The latest Ethereum 1x meeting, which was cond...</td>\n",
       "      <td>['latest', 'ethereum', '1x', 'meeting', 'condu...</td>\n",
       "      <td>The latest Ethereum 1x meeting, which was cond...</td>\n",
       "      <td>[the, latest, ethereum, meeting, which, was, c...</td>\n",
       "      <td>[the, latest, ethereum, meeting, which, was, c...</td>\n",
       "      <td>[the, latest, ethereum, meeting, which, was, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time                                               body  \\\n",
       "0  1543622368  CoinSpeakerProBit: Professional Digital Curren...   \n",
       "1  1543620634  Today in crypto heard rumors of Satoshi's retu...   \n",
       "2  1543620300  Crashes in the crypto market resulted in spike...   \n",
       "3  1543620202  SEC Chairman Jay Clayton has claimed that bitc...   \n",
       "4  1543618813  There are four different Ethereum working grou...   \n",
       "5  1543617903  On Nov. 29, Sirin Labs announced the commercia...   \n",
       "6  1543616700  Switzerlands oldest university, the University...   \n",
       "7  1543616305  The newer HBUS cryptocurrency exchange has tak...   \n",
       "8  1543616240                          Having met the U.S. Se...   \n",
       "9  1543615852  The latest Ethereum 1x meeting, which was cond...   \n",
       "\n",
       "                                    stopword_reviews  \\\n",
       "0  ['coinspeakerprobit', 'professional', 'digital...   \n",
       "1  ['today', 'crypto', 'heard', 'rumors', 'satosh...   \n",
       "2  ['crashes', 'crypto', 'market', 'resulted', 's...   \n",
       "3  ['sec', 'chairman', 'jay', 'clayton', 'claimed...   \n",
       "4  ['four', 'different', 'ethereum', 'working', '...   \n",
       "5  ['nov', '29', 'sirin', 'labs', 'announced', 'c...   \n",
       "6  ['switzerlands', 'oldest', 'university', 'univ...   \n",
       "7  ['newer', 'hbus', 'cryptocurrency', 'exchange'...   \n",
       "8  ['met', 'u', 'securities', 'exchange', 'commis...   \n",
       "9  ['latest', 'ethereum', '1x', 'meeting', 'condu...   \n",
       "\n",
       "                                         body_as_str  \\\n",
       "0  CoinSpeakerProBit: Professional Digital Curren...   \n",
       "1  Today in crypto heard rumors of Satoshi's retu...   \n",
       "2  Crashes in the crypto market resulted in spike...   \n",
       "3  SEC Chairman Jay Clayton has claimed that bitc...   \n",
       "4  There are four different Ethereum working grou...   \n",
       "5  On Nov. 29, Sirin Labs announced the commercia...   \n",
       "6  Switzerlands oldest university, the University...   \n",
       "7  The newer HBUS cryptocurrency exchange has tak...   \n",
       "8                          Having met the U.S. Se...   \n",
       "9  The latest Ethereum 1x meeting, which was cond...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [coinspeakerprobit, professional, digital, cur...   \n",
       "1  [today, crypto, heard, rumors, satoshi, return...   \n",
       "2  [crashes, the, crypto, market, resulted, spike...   \n",
       "3  [sec, chairman, jay, clayton, has, claimed, th...   \n",
       "4  [there, are, four, different, ethereum, workin...   \n",
       "5  [nov , sirin, labs, announced, the, commercial...   \n",
       "6  [switzerlands, oldest, university, the, univer...   \n",
       "7  [the, newer, hbus, cryptocurrency, exchange, h...   \n",
       "8  [having, met, the, u s , securities, and, exch...   \n",
       "9  [the, latest, ethereum, meeting, which, was, c...   \n",
       "\n",
       "                                   review_lemmatized  \\\n",
       "0  [coinspeakerprob, professional, digital, curre...   \n",
       "1  [today, crypto, heard, rumor, satosh, return, ...   \n",
       "2  [crash, the, crypto, market, resulted, spik, m...   \n",
       "3  [sec, chairman, jay, clayton, has, claimed, th...   \n",
       "4  [ther, are, four, different, ethereum, working...   \n",
       "5  [nov , sirin, lab, announced, the, commercial,...   \n",
       "6  [switzerland, oldest, university, the, univers...   \n",
       "7  [the, new, hbus, cryptocurrency, exchang, has,...   \n",
       "8  [having, met, the, u s , securit, and, exchang...   \n",
       "9  [the, latest, ethereum, meeting, which, was, c...   \n",
       "\n",
       "                                     body_lemmatized  \n",
       "0  [coinspeakerprob, professional, digital, curre...  \n",
       "1  [today, crypto, heard, rumor, satosh, return, ...  \n",
       "2  [crash, the, crypto, market, resulted, spik, m...  \n",
       "3  [sec, chairman, jay, clayton, has, claimed, th...  \n",
       "4  [ther, are, four, different, ethereum, working...  \n",
       "5  [nov , sirin, lab, announced, the, commercial,...  \n",
       "6  [switzerland, oldest, university, the, univers...  \n",
       "7  [the, new, hbus, cryptocurrency, exchang, has,...  \n",
       "8  [having, met, the, u s , securit, and, exchang...  \n",
       "9  [the, latest, ethereum, meeting, which, was, c...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv (r'cryptodata_test_lemmatized', index=False, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Après une désastreuse aventure au Bois franc, quel plaisir de retourner au lac d'Ailette..en plus les cottages ont été rénovés avec goût, Partis début août sous la canicule, nous avons apprécié les plaisirs de l'aquamondo malgré l'affluence,(peut être faudrait-il que les personnes ne séjournant pas dans le parc n'aient plus accès aux installations quand celui-ci est déjà complet - 1700 personnes dans la piscine c'est vraiment beaucoup trop)Nous avons loué un cottage premium bord de lac . Nous avons aimés nos soirées tranquilles sur la terrasse; beaucoup moins la chaleur dans le cottage (ventilation à prévoir)  celui-ci était très propre à notre arrivée et les quelques soucis rencontrés (pas de pile dans la télécommande, sèche-linge cassé) ont été très vite réglés par un personnel compétant  De quoi bien commencer notre séjour. Un bel endroit où nous reviendrons certainement l'été prochain\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "X_HEC_Session_3_Notebook_2_stemming.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
